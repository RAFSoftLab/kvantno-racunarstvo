{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429de0aa",
   "metadata": {},
   "source": [
    "## 4-Qubit 2-Layer Quantum Born Machine (QBM) for Traffic Simulation\n",
    "\n",
    "A Quantum Born Machine (QBM) is a generative quantum model that learns to reproduce a target probability distribution using the Born rule.\n",
    "![4-qubit-2-layer-QBM](images/4-qubit-2-layer-QBM.png)\n",
    "In this lab, the target data represents a typical urban traffic pattern with two peaks: morning and late afternoon, corresponding to rush-hour traffic flows. The QBM uses a parameterized 4-qubit quantum circuit to approximate this distribution.\n",
    "\n",
    "### Loss Function\n",
    "The QBM minimizes the Jensen–Shannon (JS) divergence, which symmetrizes the Kullback–Leibler divergence:\n",
    "\n",
    "$$\n",
    "JS(P \\parallel Q) = \\tfrac{1}{2} KL(P \\parallel M) + \\tfrac{1}{2} KL(Q \\parallel M)\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $P$ is the model distribution,  \n",
    "- $Q$ is the target distribution, and  \n",
    "- $M = \\tfrac{1}{2}(P+Q)$ is the midpoint distribution.\n",
    "\n",
    "The complete formula is:\n",
    "\n",
    "$$\n",
    "JS(P \\parallel Q) =\n",
    "\\tfrac{1}{2} \\sum_x P(x) \\cdot \\log \\frac{P(x)}{M(x)} \\;+\\;\n",
    "\\tfrac{1}{2} \\sum_x Q(x) \\cdot \\log \\frac{Q(x)}{M(x)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "M(x) = \\tfrac{1}{2} \\big(P(x) + Q(x)\\big)\n",
    "$$\n",
    "\n",
    "This provides a bounded, symmetric measure of dissimilarity between the model and target.\n",
    "\n",
    "## SPSA Optimization\n",
    "The parameters of the circuit are updated using SPSA (Simultaneous Perturbation Stochastic Approximation), a gradient-free optimization algorithm. At each step, SPSA perturbs all parameters with random noise, estimates the gradient using just two function evaluations, and adjusts parameters accordingly. This makes SPSA particularly efficient for noisy quantum simulations.\n",
    "\n",
    "#### SPSA Algorithm Steps\n",
    "1. Initialization:  \n",
    "   - Copy the initial parameters $\\theta_0$ into a working variable $\\theta$.  \n",
    "   - Create an empty list `losses` to record loss values over iterations.\n",
    "\n",
    "2. Iterative Optimization Loop (repeat for $max\\_iter$ steps):\n",
    "   - Generate random perturbation $\\delta$:  \n",
    "     Create a perturbation vector $\\delta$, where each component is randomly chosen as $+1$ or $-1$.  \n",
    "   - Perturb parameters:  \n",
    "     Create two perturbed versions of $\\theta$:  \n",
    "     $$\n",
    "     \\theta^+ = \\theta + c \\delta, \\quad \\theta^- = \\theta - c \\delta\n",
    "     $$\n",
    "   - Evaluate loss at perturbed points:  \n",
    "     Compute the loss at both perturbed parameter sets:  \n",
    "     $$\n",
    "     L^+ = loss\\_fn(\\theta^+), \\quad L^- = loss\\_fn(\\theta^-)\n",
    "     $$\n",
    "   - Estimate the gradient:  \n",
    "     Use the finite-difference approximation to estimate the gradient:  \n",
    "     $$\n",
    "     grad\\_estimate = \\frac{L^+ - L^-}{2c} \\, \\delta\n",
    "     $$\n",
    "     This requires only two loss evaluations, regardless of the size of $\\theta$.  \n",
    "   - Update parameters:  \n",
    "     Apply the estimated gradient to update the parameters:  \n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta - \\alpha \\cdot grad\\_estimate\n",
    "     $$\n",
    "   - Evaluate and store current loss:  \n",
    "     Compute the loss at the new $\\theta$ and append it to the list `losses`.\n",
    "\n",
    "3. Return Results:  \n",
    "   Return the final optimized parameter vector and the full list of loss values for convergence plotting.\n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "Build a 4-qubit, 2-layer Quantum Born Machine (QBM) that learns the given traffic probability distribution.  \n",
    "The circuit is trained by minimizing the Jensen–Shannon (JS) divergence between the model output and the target distribution, using Simultaneous Perturbation Stochastic Approximation (SPSA) as the optimization routine.\n",
    "\n",
    "\n",
    "### Expected Output\n",
    "- A plot showing the JS loss convergence over SPSA iterations.  \n",
    "- The final learned distribution from the QBM compared side-by-side with the target urban traffic distribution.  \n",
    "- A quantum circuit diagram of the optimized QBM.\n",
    "\n",
    "\n",
    "## Experimentation\n",
    "- Vary the number of shots in sampling to explore the effect of measurement noise.  \n",
    "- Increase the number of layers or change entanglement patterns to test circuit expressiveness.  \n",
    "- Compare SPSA with other optimizers (e.g., COBYLA, gradient descent) and observe convergence behavior.  \n",
    "- Replace the JS divergence with other loss functions.\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "- Simultaneous Perturbation Stochastic Approximation (SPSA):  \n",
    "  Spall, J. C. (1992). *Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.* IEEE Transactions on Automatic Control, 37(3), 332–341.  \n",
    "\n",
    "- Jensen–Shannon (JS) Divergence:  \n",
    "  Lin, J. (1991). *Divergence measures based on the Shannon entropy.* IEEE Transactions on Information Theory, 37(1), 145–151.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.visualization import circuit_drawer\n",
    "from qiskit_aer import AerSimulator\n",
    "\n",
    "\n",
    "# -- Target distribution to learn --\n",
    "p_dist = [0.03, 0.05, 0.07, 0.10, 0.12, 0.08, 0.05, 0.04,\n",
    "          0.04, 0.06, 0.10, 0.08, 0.05, 0.05, 0.04, 0.04] \n",
    "p_target = {format(i, '04b'): p for i, p in enumerate(p_dist)}\n",
    "\n",
    "# -- Create QBM circuit with 8 parameters --\n",
    "def create_qbm_circuit(theta):\n",
    "    qc = QuantumCircuit(4)\n",
    "    \n",
    "    # First Ry layer\n",
    "    for i in range(4):\n",
    "        qc.ry(theta[i], i)\n",
    "        \n",
    "    # First CX layer\n",
    "    qc.cx(0, 1)\n",
    "    qc.cx(1, 2)\n",
    "    qc.cx(2, 3)\n",
    "    qc.cx(3, 0)\n",
    "    qc.barrier()\n",
    "\n",
    "    # Second Ry layer\n",
    "    for i in range(4):\n",
    "        qc.ry(theta[4 + i], i)\n",
    "\n",
    "    # Second CX layer (entangle differently if needed)\n",
    "    qc.cx(0, 1)\n",
    "    qc.cx(1, 2)\n",
    "    qc.cx(2, 3)\n",
    "    qc.cx(3, 0)\n",
    "\n",
    "    qc.measure_all()\n",
    "    return qc\n",
    "    \n",
    "# -- Sample from QBM circuit using AerSimulator --\n",
    "def sample_qbm(theta, shots=1000):\n",
    "    qc = create_qbm_circuit(theta)\n",
    "    simulator = AerSimulator()\n",
    "    result = simulator.run(qc, shots=shots).result()\n",
    "    counts = result.get_counts()\n",
    "    total = sum(counts.values())\n",
    "    p_model = {bit: count / total for bit, count in counts.items()}\n",
    "    return p_model\n",
    "\n",
    "# -- Jensen–Shannon (JS) divergence loss function  --\n",
    "def js_loss(theta):\n",
    "    p_model = sample_qbm(theta)\n",
    "    eps = 1e-10  # small value to avoid log(0)\n",
    "    js = 0.0\n",
    "    for k in p_target:\n",
    "        p = p_model.get(k, eps)\n",
    "        q = p_target[k]\n",
    "        m = 0.5 * (p + q)\n",
    "        if p > 0:\n",
    "            js += 0.5 * p * np.log(p / m)\n",
    "        if q > 0:\n",
    "            js += 0.5 * q * np.log(q / m)\n",
    "    return js\n",
    "\n",
    "# -- Simultaneous Perturbation Stochastic Approximation  (SPSA) optimization  --\n",
    "def spsa(loss_fn, theta0, alpha=0.2, c=0.1, max_iter=200):\n",
    "    theta = np.copy(theta0)\n",
    "    n = len(theta)\n",
    "    losses = []\n",
    "\n",
    "    for step in range(max_iter):\n",
    "        delta = 2 * np.random.randint(0, 2, size=n) - 1\n",
    "        theta_plus = theta + c * delta\n",
    "        theta_minus = theta - c * delta\n",
    "        loss_plus = loss_fn(theta_plus)\n",
    "        loss_minus = loss_fn(theta_minus)\n",
    "        grad_estimate = (loss_plus - loss_minus) / (2 * c) * delta\n",
    "        theta = theta - alpha * grad_estimate\n",
    "        current_loss = loss_fn(theta)\n",
    "        losses.append(current_loss)\n",
    "    return theta, losses\n",
    "\n",
    "# -----------------------------------------------\n",
    "#                main program\n",
    "# -----------------------------------------------\n",
    "\n",
    "# -- Run SPSA optimization --\n",
    "theta0 = np.random.uniform(0, 2 * np.pi, 8)\n",
    "theta_opt, losses = spsa(js_loss, theta0, max_iter=250)\n",
    "\n",
    "# -- Plot convergence of loss --\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, marker='.', linestyle='-')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"QBM Loss Convergence Over Iterations\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- Final learned distribution --\n",
    "p_qbm = sample_qbm(theta_opt)\n",
    "qc = create_qbm_circuit(theta_opt)\n",
    "display(circuit_drawer(qc, style=\"bw\", output=\"mpl\"))\n",
    "\n",
    "# -- Plot learned vs. target distribution --\n",
    "bitstrings = sorted(set(p_target.keys()).union(p_qbm.keys()))\n",
    "target_probs = [p_target.get(b, 0) for b in bitstrings]\n",
    "qbm_probs = [p_qbm.get(b, 0) for b in bitstrings]\n",
    "\n",
    "x = np.arange(len(bitstrings))\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(x - bar_width / 2, target_probs, width=bar_width, label=\"Target\")\n",
    "plt.bar(x + bar_width / 2, qbm_probs, width=bar_width, label=\"QBM Output\")\n",
    "plt.xticks(x, bitstrings, rotation=45, ha='right')\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Target vs QBM Output Distribution\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit-2.0.2",
   "language": "python",
   "name": "qiskit-2.0.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
