{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21858f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimal 2-qubit QBM\n",
    "\n",
    "A Quantum Born Machine (QBM) is a generative quantum machine learning model that learns probability distributions using the Born rule. In essence, the QBM is given a target distribution and must adjust its quantum circuit parameters so that the distribution of its measurement outcomes becomes similar to the given one. This lab demonstrates the smallest non-trivial QBM, built with two qubits and a single variational parameter.\n",
    "\n",
    "![2-qubit-QBM](images/2-qubit-QBM.png)\n",
    "\n",
    "The model circuit consists of:\n",
    "\n",
    "1. An `RY(θ)` rotation on qubit 0.  \n",
    "2. A `CX(0→1)` gate to entangle the two qubits.  \n",
    "3. Measurement in the computational basis.  \n",
    "\n",
    "The resulting state is:\n",
    "\n",
    "$$\n",
    "\\cos\\!\\left(\\tfrac{\\theta}{2}\\right)\\lvert 00\\rangle \\;+\\; \\sin\\!\\left(\\tfrac{\\theta}{2}\\right)\\lvert 11\\rangle\n",
    "$$\n",
    "\n",
    "Hence, measurement outcomes are restricted to:\n",
    "\n",
    "$$\n",
    "P_{\\text{model}}(00) = \\cos^2\\!\\left(\\tfrac{\\theta}{2}\\right), \\qquad\n",
    "P_{\\text{model}}(11) = \\sin^2\\!\\left(\\tfrac{\\theta}{2}\\right).\n",
    "$$\n",
    "\n",
    "This makes the model capable of representing any binary distribution supported on $\\{00,11\\}$.\n",
    "\n",
    "## Code components\n",
    "\n",
    "- **`vqc(theta, plot=False)`**  \n",
    "  Builds the variational quantum circuit, executes it on the simulator, and returns the measured probability distribution.  \n",
    "\n",
    "- **Loss functions**  \n",
    "  The goal is to compare the model distribution $p_{\\text{model}}$ against the target distribution $p_{\\text{target}}$.  \n",
    "\n",
    "  - `l1_loss(p_model, t_dist)`  \n",
    "    Computes the $\\ell_1$ (Manhattan) distance:  \n",
    "    $$\n",
    "    L_1(p_{\\text{model}}, p_{\\text{target}}) \\;=\\; \n",
    "    \\sum_{x} \\big|\\, p_{\\text{model}}(x) - p_{\\text{target}}(x)\\,\\big|\n",
    "    $$\n",
    "  \n",
    "  - `l2_loss(p_model, t_dist)`  \n",
    "    Computes the $\\ell_2$ (Euclidean) distance:  \n",
    "    $$\n",
    "    L_2(p_{\\text{model}}, p_{\\text{target}}) \\;=\\;\n",
    "    \\sqrt{\\;\\sum_{x} \\big( p_{\\text{model}}(x) - p_{\\text{target}}(x) \\big)^2}\n",
    "    $$\n",
    "\n",
    "- **`basic_optimizer(loss_fn, t_dist, step_size, max_iter)`**  \n",
    "  Performs a naive optimization loop:  \n",
    "  - Starts at $\\theta = 0$.  \n",
    "  - Increases $\\theta$ step by step.  \n",
    "  - Evaluates the model distribution and its loss compared to the target.  \n",
    "  - Stops when the loss starts to increase.  \n",
    "\n",
    "---\n",
    "\n",
    "## Task\n",
    "\n",
    "1. Run the notebook to optimize $\\theta$ for the default target distribution:  \n",
    "   `t_dist = {'00': 0.3, '11': 0.7}`  \n",
    "\n",
    "2. Observe how the optimizer adjusts $\\theta$ to minimize the loss.  \n",
    "3. Replace `t_dist` with other valid probability distributions on $\\{00,11\\}$ (for example, `{'00': 0.5, '11': 0.5}`) and compare results.  \n",
    "4. Try both `l1_loss` and `l2_loss` to see how they behave differently.  \n",
    "\n",
    "## Expected output\n",
    "\n",
    "- A loss vs. $\\theta$ plot showing how the loss decreases as the parameter is optimized.  \n",
    "- A circuit diagram of the final trained QBM.  \n",
    "- Overlayed bar plots of $p_{\\text{model}}$ and $t_{\\text{dist}}$ for a quick visual check of the fit. \n",
    "- A printout of the optimized parameter value and the number of iterations.  \n",
    "\n",
    "## Experimentation\n",
    "\n",
    "- Extend the circuit so it can also place probability on $\\lvert 01\\rangle$ and $\\lvert 10\\rangle$.  \n",
    "- Replace the naive line search with a simple gradient-based update on $\\theta$.  \n",
    "- Study how different shot counts affect the learned $\\theta$ and final loss.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92719d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.visualization import plot_distribution\n",
    "from qiskit.visualization import circuit_drawer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vqc(theta, plot=False):\n",
    "    \"\"\"\n",
    "    Creates and runs a simple 2-qubit variational quantum circuit.\n",
    "\n",
    "    Applies an RY rotation by angle `theta` on qubit 0 followed by a CNOT gate to entangle qubit 0 and 1.\n",
    "    The circuit is measured, optionally plotted, and executed on a simulator.\n",
    "\n",
    "    Args:\n",
    "        theta (float): Rotation angle for the RY gate on qubit 0.\n",
    "        plot (bool): If True, displays the circuit diagram.\n",
    "\n",
    "    Returns:\n",
    "        dict: Probability distribution of measurement outcomes (bitstrings).\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(2,2)\n",
    "    \n",
    "    # set state       \n",
    "    qc.ry(theta, 0)\n",
    "    qc.cx(0, 1)\n",
    "\n",
    "    qc.barrier()\n",
    "    qc.measure(range(2), range(2))\n",
    "    if plot:\n",
    "        display(circuit_drawer(qc, output=\"mpl\"))\n",
    "    \n",
    "    # -- run the program on the simulator --\n",
    "    simulator = AerSimulator()\n",
    "    result = simulator.run(qc, shots=1000).result()\n",
    "    counts = result.get_counts(qc)\n",
    "\n",
    "    # convert counts to probability distribution\n",
    "    total = sum(counts.values())\n",
    "    p_model = {k: v / total for k, v in counts.items()}\n",
    "    \n",
    "    return p_model\n",
    "\n",
    "def l1_loss(p_model, t_dist):\n",
    "    \"\"\"\n",
    "    Computes the L1 loss (Manhattan distance) between two probability distributions.\n",
    "\n",
    "    The L1 loss is the sum of absolute differences between corresponding probabilities \n",
    "    in the model and target distributions. Missing keys are treated as zero.\n",
    "\n",
    "    Args:\n",
    "        p_model (dict): The predicted/model probability distribution.\n",
    "        t_dist (dict): The target/reference probability distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: Total L1 loss between the two distributions.\n",
    "    \"\"\"\n",
    "    keys = set(p_model.keys()).union(t_dist.keys())\n",
    "    loss = 0.0\n",
    "    for k in keys:\n",
    "        model_prob = p_model.get(k, 0.0)    # use 0.0 if key not found\n",
    "        target_prob = t_dist.get(k, 0.0)    # use 0.0 if key not found\n",
    "        diff = abs(model_prob - target_prob)\n",
    "        loss += diff\n",
    "    return loss\n",
    "\n",
    "def l2_loss(p_model, t_dist):\n",
    "    \"\"\"\n",
    "    Computes the L2 loss (Euclidean distance) between two probability distributions.\n",
    "\n",
    "    The L2 loss is the square root of the sum of squared differences between corresponding \n",
    "    probabilities in the model and target distributions. Missing keys are treated as zero.\n",
    "\n",
    "    Args:\n",
    "        p_model (dict): The predicted/model probability distribution.\n",
    "        t_dist (dict): The target/reference probability distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: Total L2 loss between the two distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    keys = set(p_model.keys()).union(t_dist.keys())\n",
    "    loss = 0.0\n",
    "    for k in keys:\n",
    "        model_prob = p_model.get(k, 0.0)    # use 0.0 if key not found\n",
    "        target_prob = t_dist.get(k, 0.0)    # use 0.0 if key not found\n",
    "        diff = model_prob - target_prob\n",
    "        loss += diff ** 2\n",
    "    return loss ** 0.5   # square root for L2\n",
    "\n",
    "\n",
    "def basic_optimizer(loss_fn, t_dist, step_size=0.1, max_iter=100):\n",
    "    \"\"\"\n",
    "    Optimizes the rotation angle theta to minimize the loss \n",
    "    between the VQC output distribution and a target distribution.\n",
    "\n",
    "    The function incrementally increases theta, evaluates the VQC output, \n",
    "    and computes the loss against the target. Optimization stops when \n",
    "    the loss increases, indicating a minimum has been passed.\n",
    "\n",
    "    Args:\n",
    "        loss_fn: loss function name\n",
    "        t_dist (dict): Target probability distribution.\n",
    "        step_size (float): Increment for theta in each iteration.\n",
    "        max_iter (int): Maximum number of optimization steps.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lists of theta values and corresponding losses.\n",
    "    \"\"\"\n",
    "    theta = 0.0\n",
    "    losses = []\n",
    "    thetas = []\n",
    "\n",
    "    prev_loss = None\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # get model distribution from VQC at current theta\n",
    "        p_model = vqc(theta)\n",
    "\n",
    "        # compute L1 loss\n",
    "        loss = loss_fn(p_model, t_dist)\n",
    "\n",
    "        # check if loss starts increasing\n",
    "        if prev_loss is not None and loss > prev_loss:\n",
    "            losses.pop() # discard the last guess since it performed worse \n",
    "            thetas.pop()\n",
    "            break\n",
    "            \n",
    "        # store for graphing\n",
    "        thetas.append(theta)\n",
    "        losses.append(loss)\n",
    "\n",
    "        prev_loss = loss\n",
    "        theta += step_size\n",
    "\n",
    "    return thetas, losses, i\n",
    "\n",
    "# ------------------------------------------------\n",
    "#                 main program\n",
    "# ------------------------------------------------\n",
    "# target distribution\n",
    "t_dist = {'00': 0.3, '11': 0.7} \n",
    "thetas, losses, iter = basic_optimizer(l2_loss, t_dist)\n",
    "\n",
    "# plot results\n",
    "plt.plot(thetas, losses, marker='o')\n",
    "plt.xlabel(\"θ\")\n",
    "plt.ylabel(\"L2 Loss\")\n",
    "plt.title(\"Loss vs θ\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# display circuit and results with final paramter\n",
    "model_dist = vqc(thetas[-1], True)\n",
    "display(\n",
    "    plot_distribution(\n",
    "        [t_dist, model_dist],\n",
    "        title=\"Target vs Model Distribution\",\n",
    "        legend=[\"Target (requested)\", \"Model\"]\n",
    "    )\n",
    ")\n",
    "print(f\"Optimization finished after {iter} iterations with θ = {thetas[-1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit-2.0.2",
   "language": "python",
   "name": "qiskit-2.0.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
